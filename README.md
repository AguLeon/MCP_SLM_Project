# OpenMCP notebook to create resources in ChameleonCloud

Repository for all the work related to our research paper on MCPs (Model Context Protocols).

## Overview

This repository contains Jupyter notebooks and resources for setting up and running experiments on [Chameleon Cloud](https://chameleoncloud.org/), a configurable experimental environment for large-scale cloud research.

## Prerequisites

Before using these notebooks, ensure you have:

1. **Chameleon Cloud Account** - Register at [chameleoncloud.org](https://chameleoncloud.org/)
2. **Project Membership** - Be part of a Chameleon project (project ID in format `CHI-XXXXX`)
3. **SSH Key Pair** - Create and upload an SSH key pair to your Chameleon project
4. **JupyterHub Access** - Access to Chameleon's JupyterHub environment

## Repository Structure

```
.
├── README.md
├── start_edge_llm.ipynb                    # Deploy LLMs on edge devices (CHI@Edge)
├── start_run_instace_directly.ipynb        # Create instances from images (KVM@TACC, CHI@TACC, CHI@UC)
└── start_run_instance_from_volume.ipynb    # Create instances from volumes (KVM@TACC)
```

## Getting Started

### Option 1: Create Instance Directly from Image

Use `start_run_instace_directly.ipynb` for standard deployments:

1. Open the notebook in Chameleon JupyterHub
2. Select your project and site (default: `KVM@TACC`)
3. Configure your resource names and SSH key:
   ```python
   exp_name = "OpenMCP"  # Resource prefix
   key_name = "id_rsa"   # Your SSH key pair name
   ```
4. Choose your flavor (instance type):
   ```python
   flavor_name = "m1.xxlarge"  # CPU-only
   # or "gpu_p100" for GPU instances
   ```
5. Select your image:
   ```python
   image_name = "CC-Ubuntu24.04-CUDA"  # For GPU
   # or "CC-Ubuntu24.04" for CPU-only
   ```
6. Run all cells to create and configure your instance

### Option 2: Create Instance from Volume (Large Models)

Use `start_run_instance_from_volume.ipynb` when working with large models (e.g., Qwen3-vl:235b):

1. This method provides up to 1TB of volume storage (KVM@TACC has a limit of 1TB)
2. Only available on `KVM@TACC`
3. Creates a persistent bootable volume that survives instance deletion
4. Recommended for models that won't fit in default instance storage

### Option 3: Deploy on Edge Devices

Use `start_edge_llm.ipynb` for edge deployment:

1. Select `CHI@Edge` as your site
2. Deploys Ollama container on edge hardware
3. Supports NVIDIA GPU runtime

## Instance Configuration

### Sample Available Flavors (Instance Types)

| Flavor | Description | Use Case |
|--------|-------------|----------|
| `m1.xxlarge` | Large CPU instance | Development, small models |
| `gpu_p100` | P100 GPU | Medium-sized models |
| `g1.h100.pci.1` | H100 GPU | Large models, high performance |

### Available Images

| Image | Description |
|-------|-------------|
| `CC-Ubuntu24.04` | Ubuntu 24.04 (CPU only) |
| `CC-Ubuntu24.04-CUDA` | Ubuntu 24.04 with CUDA drivers |

## Post-Instance Setup

After your instance is running, SSH into it and set up the environment:

### 1. Install Docker

```bash
curl -sSL https://get.docker.com/ | sudo sh
sudo groupadd -f docker; sudo usermod -aG docker $USER
```

Restart the instance, then verify:
```bash
docker run hello-world
```

### 2. Install NVIDIA Container Toolkit (GPU instances only)

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    
sudo apt update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

Verify GPU access:
```bash
docker run --rm --gpus all ubuntu nvidia-smi
```

### 3. Clone the MCPWorld Repository

```bash
git clone https://github.com/AguLeon/MCPWorld

cd MCPWorld
git submodule update --init PC-Canary
git submodule update --init --recursive
```

Follow the instructions in the [MCPWorld README](https://github.com/AguLeon/MCPWorld) for the test environment setup.

## Security Groups and Ports

The notebooks automatically configure the following security groups:

| Port | Purpose |
|------|---------|
| 22 | SSH access |
| 8080 | HTTP services |
| 8501 | Streamlit applications |
| 5900 | VNC |
| 6080 | noVNC |
| 11434 | Ollama API (edge only) |

## Resource Management

### Viewing Your Resources

- **Instances**: Navigate to "Experiment > [Site] > Instances" in Chameleon dashboard
- **Leases**: Navigate to "Experiment > [Site] > Leases"
- **Volumes**: Navigate to "Experiment > [Site] > Volumes"

### Deleting Resources

**Important**: Chameleon is a shared facility. Delete resources when finished.

Via notebook:
```python
s.delete()  # Delete instance
l.delete()  # Delete lease
```

Or use the Chameleon web GUI to delete instances, leases, and volumes.

## Troubleshooting

### Common Issues

1. **Lease not starting**: Check resource availability at your chosen site
2. **Cannot SSH**: Verify security groups and floating IP association
3. **GPU not detected**: Ensure you're using a CUDA image and GPU flavor
4. **Volume not available**: Wait for volume status to show "available" before using

### Getting Help

- [Chameleon Cloud Documentation](https://chameleoncloud.readthedocs.io/)
- [Chameleon Support](https://chameleoncloud.org/user/help/)

## Dependencies

The notebooks require the following Python packages (pre-installed in Chameleon JupyterHub):

- `python-chi` - Chameleon Python library
- `python-novaclient` - OpenStack Compute client
- `python-neutronclient` - OpenStack Networking client
- `python-cinderclient` - OpenStack Block Storage client

## License

See [LICENSE](LICENSE) file for details.

## Acknowledgments

Notebooks adapted from "Hello, Chameleon" by Fraida Fund:
- [Original Tutorial](https://www.chameleoncloud.org/experiment/share/a10a1b51-51d7-4c6e-ba83-010a5cf759d6)

<!-- ## Citation -->
<!---->
<!-- If you use this repository in your research, please cite: -->
<!---->
<!-- ```bibtex -->
<!-- @misc{mcp_slm_project, -->
<!--   title={MCP and SLMs Benchmarking Research Project}, -->
<!--   author={AguLeon, ARNiroula}, -->
<!--   year={2026}, -->
<!--   url={https://github.com/AguLeon/MCP_SLM_Project} -->
<!-- } -->
<!-- ``` -->
